{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32364f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.core.display import Markdown\n",
    "from IPython.display import  display\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the environment variables are set\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "if anthropic_api_key is None:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY is not set\")\n",
    "if google_api_key is None:\n",
    "    raise ValueError(\"GOOGLE_API_KEY is not set\")\n",
    "\n",
    "print(openai_api_key[0:5])\n",
    "print(anthropic_api_key[0:5])\n",
    "print(google_api_key[0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2adf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging , nuanced question that I can ask a number of LLMs to evaluate their intelligence.\"\n",
    "request+= \" Answer only with the question , no explanaition.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=openai_api_key)\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gpt Model\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "repsonse = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = repsonse.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic model\n",
    "model_name = \"claude-3-7-sonnet-20250219\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini model\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923431b",
   "metadata": {},
   "source": [
    "## How to use Ollama\n",
    "\n",
    "Ollama is a tool that allows you to run large language models locally on your machine, providing an OpenAI-compatible API endpoint for easy integration.\n",
    "\n",
    "### Step 1: Installation\n",
    "1. Visit [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Download the Windows version\n",
    "3. Run the installer and follow the instructions\n",
    "4. Restart your terminal/command prompt after installation\n",
    "\n",
    "### Step 2: Start the server\n",
    "After installation, Ollama usually starts automatically as a background service. To verify it's running:\n",
    "- Visit [http://localhost:11434](http://localhost:11434) \n",
    "- You should see the message: \"Ollama is running\"\n",
    "\n",
    "### Step 3: Download and use models\n",
    "```bash\n",
    "# Download a model (this may take several minutes)\n",
    "ollama pull llama3.2\n",
    "\n",
    "# Run a model interactively\n",
    "ollama run llama3.2\n",
    "\n",
    "# Test with a quick prompt\n",
    "ollama run llama3.2 \"Hello, how are you?\"\n",
    "```\n",
    "\n",
    "## Essential Ollama Commands\n",
    "\n",
    "### Model Management\n",
    "- `ollama pull <model_name>` - Download a specific model from the library\n",
    "- `ollama ls` or `ollama list` - List all installed models with their sizes\n",
    "- `ollama rm <model_name>` - Remove a specific model to free up disk space\n",
    "- `ollama show <model_name>` - Display detailed information about a model\n",
    "\n",
    "### Server Management\n",
    "- `ollama serve` - Start the Ollama server manually (usually not needed)\n",
    "- `ollama ps` - Show currently running models and their status\n",
    "- `ollama stop <model_name>` - Stop a specific running model\n",
    "\n",
    "### Interactive Usage\n",
    "- `ollama run <model_name>` - Start an interactive chat session with a model\n",
    "- `ollama run <model_name> \"your prompt\"` - Get a single response from a model\n",
    "\n",
    "### Popular Models to Try\n",
    "- `llama3.2` - Latest Llama model (3B or 1B parameters)\n",
    "- `llama3.2:70b` - Larger Llama model for better performance\n",
    "- `mistral` - Efficient 7B parameter model\n",
    "- `codellama` - Specialized for code generation\n",
    "- `gemma2` - Google's open model\n",
    "\n",
    "### API Usage\n",
    "Once Ollama is running, you can use it with the OpenAI-compatible API at:\n",
    "- Base URL: `http://localhost:11434/v1`\n",
    "- No API key required for local usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dea73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "model_name = \"llama3.2\"\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"competitors:\", competitors)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fa156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all answers together\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"## {competitor}\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf7f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all answers together\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"## Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer\n",
    "    together += \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are a judge. You are judging competition between {len(competitors)}. You have the following answers to the question: {question}\\n\\n\n",
    "\n",
    "Your job is the evaluate each response for clarity and strenght of argument , and rank them in order of best to worst. Respond with Json , and only JSON, with \n",
    "the following format:\n",
    "{{ \"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the response from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON  with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def583e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958af5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(api_key=openai_api_key)\n",
    "response = openai.chat.completions.create(  \n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=judge_messages\n",
    ")\n",
    "\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = json.loads(results)\n",
    "ranks  = result_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor_index = int(result.split()[-1]) - 1\n",
    "    competitor_name = competitors[competitor_index]\n",
    "    display(Markdown(f\"### Rank {index+1}: {competitor_name}\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5356195",
   "metadata": {},
   "source": [
    "## Exercise: Agentic Design Patterns Analysis\n",
    "\n",
    "### Pattern Used: **Multi-Agent Competition with Judge**\n",
    "\n",
    "The current implementation uses the **\"Multi-Agent Competition\"** pattern where:\n",
    "- **Multiple AI agents** (GPT-4o-mini, Claude, Gemini, Llama) compete on the same task\n",
    "- **Single judge agent** (GPT-3.5-turbo) evaluates and ranks all responses\n",
    "- **Comparative evaluation** allows for relative performance assessment\n",
    "\n",
    "### Benefits of this pattern:\n",
    "- Reduces bias from single model evaluation\n",
    "- Leverages different model strengths\n",
    "- Provides comprehensive performance comparison\n",
    "- Enables benchmarking across providers\n",
    "\n",
    "### Adding Another Pattern: **Reflection Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304b1f0",
   "metadata": {},
   "source": [
    "# Implementation: Adding Reflection Agent Pattern\n",
    "# The reflection agent will analyze the judge's decision and provide meta-analysis\n",
    "\n",
    "def create_reflection_agent(judge_results, competitors, answers):\n",
    "    \"\"\"\n",
    "    Reflection Agent Pattern: An agent that reflects on the evaluation process\n",
    "    and provides meta-analysis of the competition results.\n",
    "    \"\"\"\n",
    "    \n",
    "    reflection_prompt = f\"\"\"\n",
    "    You are a Reflection Agent analyzing an AI competition evaluation.\n",
    "    \n",
    "    COMPETITION DETAILS:\n",
    "    - Question evaluated: {question}\n",
    "    - Number of competitors: {len(competitors)}\n",
    "    - Models tested: {', '.join(competitors)}\n",
    "    \n",
    "    JUDGE'S RANKING: {judge_results}\n",
    "    \n",
    "    Your task is to provide a meta-analysis:\n",
    "    1. Analyze the judge's decision quality\n",
    "    2. Identify potential biases in the evaluation\n",
    "    3. Suggest improvements to the evaluation process\n",
    "    4. Comment on the fairness of the ranking\n",
    "    5. Recommend which model characteristics led to better rankings\n",
    "    \n",
    "    Provide a structured analysis in markdown format.\n",
    "    \"\"\"\n",
    "    \n",
    "    reflection_messages = [{\"role\": \"user\", \"content\": reflection_prompt}]\n",
    "    \n",
    "    # Use Claude for reflection (different from the judge)\n",
    "    claude = Anthropic(api_key=anthropic_api_key)\n",
    "    response = claude.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        messages=reflection_messages,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text\n",
    "\n",
    "# Execute Reflection Agent\n",
    "print(\"ðŸ¤” REFLECTION AGENT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "reflection_analysis = create_reflection_agent(results, competitors, answers)\n",
    "display(Markdown(reflection_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: Adding Chain-of-Thought Reasoning Pattern\n",
    "# This agent will break down the evaluation into multiple reasoning steps\n",
    "\n",
    "def create_cot_evaluator(question, answers, competitors):\n",
    "    \"\"\"\n",
    "    Chain-of-Thought Pattern: An agent that uses step-by-step reasoning\n",
    "    to evaluate responses with explicit reasoning chains.\n",
    "    \"\"\"\n",
    "    \n",
    "    cot_prompt = f\"\"\"\n",
    "    You are a Chain-of-Thought Evaluator. Evaluate each response using explicit step-by-step reasoning.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    For each response, follow this reasoning chain:\n",
    "    1. UNDERSTANDING: Does the response show understanding of the question?\n",
    "    2. ACCURACY: How factually accurate is the response?\n",
    "    3. COMPLETENESS: Does it address all aspects of the question?\n",
    "    4. CLARITY: How clear and well-structured is the response?\n",
    "    5. CREATIVITY: Does it show original thinking or insights?\n",
    "    6. PRACTICAL VALUE: How useful would this response be?\n",
    "    \n",
    "    RESPONSES TO EVALUATE:\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, (competitor, answer) in enumerate(zip(competitors, answers), 1):\n",
    "        cot_prompt += f\"\\n\\nRESPONSE {i} ({competitor}):\\n{answer}\\n\"\n",
    "    \n",
    "    cot_prompt += \"\"\"\n",
    "    \n",
    "    Please provide:\n",
    "    1. Step-by-step evaluation for each response using the 6 criteria above\n",
    "    2. Detailed reasoning for scores (1-10 scale for each criterion)\n",
    "    3. Final ranking with explicit justification\n",
    "    4. Summary of why the top response was chosen\n",
    "    \n",
    "    Format your response in clear sections for each competitor.\n",
    "    \"\"\"\n",
    "    \n",
    "    cot_messages = [{\"role\": \"user\", \"content\": cot_prompt}]\n",
    "    \n",
    "    # Use Gemini for Chain-of-Thought evaluation\n",
    "    gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "    response = gemini.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=cot_messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Execute Chain-of-Thought Evaluator\n",
    "print(\"ðŸ§  CHAIN-OF-THOUGHT EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cot_analysis = create_cot_evaluator(question, answers, competitors)\n",
    "display(Markdown(cot_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e8d4a",
   "metadata": {},
   "source": [
    "## Summary: Agentic Design Patterns Implemented\n",
    "\n",
    "### 1. **Multi-Agent Competition** (Original Pattern)\n",
    "- **Purpose**: Compare multiple AI models on the same task\n",
    "- **Implementation**: 4 different LLMs answer the same question\n",
    "- **Benefit**: Diverse perspectives and capabilities comparison\n",
    "\n",
    "### 2. **Judge Agent** (Original Pattern)  \n",
    "- **Purpose**: Impartial evaluation of competing responses\n",
    "- **Implementation**: GPT-3.5-turbo ranks all responses\n",
    "- **Benefit**: Consistent evaluation criteria\n",
    "\n",
    "### 3. **Reflection Agent** (Added Pattern)\n",
    "- **Purpose**: Meta-analysis of the evaluation process\n",
    "- **Implementation**: Claude analyzes the judge's decision quality\n",
    "- **Benefit**: Identifies biases and suggests improvements\n",
    "\n",
    "### 4. **Chain-of-Thought Evaluator** (Added Pattern)\n",
    "- **Purpose**: Explicit step-by-step reasoning in evaluation\n",
    "- **Implementation**: Gemini uses 6-criteria breakdown with reasoning\n",
    "- **Benefit**: Transparent and detailed evaluation process\n",
    "\n",
    "### Pattern Synergies:\n",
    "- **Competition + Reflection**: Ensures fair evaluation\n",
    "- **Judge + Chain-of-Thought**: Provides both quick and detailed assessments\n",
    "- **Multiple evaluators**: Reduces single-point-of-failure in evaluation\n",
    "\n",
    "This creates a robust **Multi-Agent Evaluation Ecosystem** with checks and balances! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
